from config import INDEX_PATH, META_PATH, NEWS_LIMIT
from fetcher import fetch_nasdaq_news, fetch_article_content
from summarizer import summarize
from embedder import embed
from index_manager import load_index_and_metadata, save_index_and_metadata
from search import faiss_search
import numpy as np
import pandas as pd
from langchain.docstore import InMemoryDocstore

def run():
    # ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° ë¡œë“œ
    index, df = load_index_and_metadata(INDEX_PATH, META_PATH)
    articles = fetch_nasdaq_news(limit=NEWS_LIMIT)
    print(f"ğŸ“° ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜: {len(articles)}")

    # df["date"]ë¥¼ ë¬¸ìì—´ë¡œ ê°•ì œ ë³€í™˜ (ì¤‘ë³µ ì²´í¬ ì •í™•ë„ í–¥ìƒ)
    df["date"] = df["date"].astype(str)

    # ìƒˆ ë¬¸ì„œ ì„ë² ë”© ë° docstore ì„¤ì •
    new_embeddings = []
    new_rows = []

    # ê¸°ì¡´ title+date ì¡°í•© í‚¤ë¡œ ì¤‘ë³µ ì²´í¬ìš© Set ìƒì„±
    existing_keys = set((df["title"] + "|" + df["date"]).values)

    # docstore ë° index_to_docstore_id ì´ˆê¸°í™”
    docstore = InMemoryDocstore({})  # ë¹ˆ ë¬¸ì„œ ì €ì¥ì†Œ ìƒì„±
    index_to_docstore_id = {}

    # ê¸°ì¡´ title+dateë¥¼ ê¸°ë°˜ìœ¼ë¡œ docstore ë° ì¸ë±ìŠ¤ ID ë§¤í•‘ ì„¤ì •
    for idx, row in df.iterrows():
        doc_id = f"doc_{idx}"
        doc_content = row['summary']  # ì˜ˆì‹œë¡œ summaryë¥¼ ì›ë³¸ í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
        docstore.add_document(doc_id, doc_content)  # docstoreì— ì›ë³¸ ë¬¸ì„œ ì¶”ê°€
        index_to_docstore_id[idx] = doc_id  # ì¸ë±ìŠ¤ IDì™€ ë¬¸ì„œ ID ë§¤í•‘

    # ê¸°ì‚¬ ì²˜ë¦¬ ë° ì„ë² ë”© ì¶”ê°€
    for idx, article in enumerate(articles, start=1):
        raw_title = article["title"]
        date_str = article["pub_date"][:10]  # YYYY-MM-DD
        title = f"{raw_title} [{date_str}]"  # ì œëª© + ë‚ ì§œ ì¡°í•©
        url = article["url"]

        # ì¤‘ë³µ ê²€ì‚¬ (title + date ë¬¸ìì—´ ê¸°ì¤€)
        key = f"{title}|{date_str}"
        if key in existing_keys:
            print(f"âš ï¸ ì¤‘ë³µ ê¸°ì‚¬ ìŠ¤í‚µ: {title}")
            continue

        print(f"\n[{idx}] {title}")
        print(f"ğŸ“„ URL: {url}")
        content, _ = fetch_article_content(url)
        if not content:
            print("âŒ ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨")
            continue

        summary = summarize(title, content)
        print("ğŸ“ ìš”ì•½ ì™„ë£Œ")

        # ì„ë² ë”© ìƒì„±
        vector = embed(f"{title}\n{summary}")
        new_embeddings.append(vector)
        new_rows.append({
            "id": f"article_{len(df) + len(new_rows) + 1}",
            "title": title,
            "date": date_str,
            "summary": summary
        })

    # ìƒˆ ì„ë² ë”© ì¶”ê°€ ë° ì¸ë±ìŠ¤ ì €ì¥
    if new_embeddings:
        # FAISS ì¸ë±ìŠ¤ì— ë²¡í„° ì¶”ê°€
        index.add(np.array(new_embeddings))
        df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)  # ë©”íƒ€ë°ì´í„° ì¶”ê°€

        # ìƒˆ ë¬¸ì„œ ì •ë³´ docstoreì— ì¶”ê°€
        for i, row in enumerate(new_rows, start=len(df) - len(new_rows)):
            doc_id = f"doc_{i}"
            doc_content = row["summary"]
            docstore.add_document(doc_id, doc_content)  # docstoreì— ì›ë³¸ ë¬¸ì„œ ì¶”ê°€
            index_to_docstore_id[i] = doc_id  # ì¸ë±ìŠ¤ IDì™€ ë¬¸ì„œ ID ë§¤í•‘

        # ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° ì €ì¥
        save_index_and_metadata(index, df, INDEX_PATH, META_PATH)
        print(f"\nâœ… {len(new_rows)}ê°œì˜ ê¸°ì‚¬ ì €ì¥ ì™„ë£Œ")
    else:
        print("â„¹ï¸ ìƒˆë¡œ ì €ì¥ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # faiss_search(index, df)  # ì¶”ê°€ì ìœ¼ë¡œ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ ì—¬ê¸°ë¥¼ í™œì„±í™”

if __name__ == "__main__":
    run()
